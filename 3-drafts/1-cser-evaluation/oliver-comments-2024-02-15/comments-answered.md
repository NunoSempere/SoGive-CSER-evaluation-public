* Page #1: minor general comment: If sharing publicly or with several CSER people, I recommend having a version with more formal language ("mouthful", "anyways") and less blunt language about CSER people ("clueless")

Answer: Yes, I agree that this would have to be cleaned up significantly. 

* Page #2 (1.1. Introduction and positive aspects of CSER):

Comment: (minor) I'm not sure I understand the precise distinction you're making here - if humanity is sufficiently crippled, then that would be an existential catastrophe (at least in the Bostrom / Ord sense). Maybe you mean 'crippled but not permanently or not that badly'?

Answer: I'm pointing that "sufficiently" in your previous sentence is fuzzy, making "existential catastrophe" also fuzzy as well. For instance, it's very unclear what number of people killed out of the current 7-8 billion would constitute an existential catastrophe. Despite this, I think the existential risk concept is useful, and so I use it over the more tightly defined extinction risk.

* Page #3 (1.2. Neutral aspects of CSER): Nice point

* Page #3 (1.2. Neutral aspects of CSER): (minor) New readers would prob benefit from the explicit context that he spoke a lot about secrecy risks in the initial interview

Answer: Agree

* Page #3 (1.3. Negative aspects of CSER): Good point

* Page #5 (1.4. Synthesis: A tricky and potentially exciting opportunity for impact): Seems like a good synthesis, and I agree with the reasoning + choices of what to discuss in section 1

* Page #5 (2. Summary of a threshold approach for medium-sized donors): (These still seem like good thresholds, and I appreciate that you refreshed my memory of them here)

* Page #5 (2. Summary of a threshold approach for medium-sized donors): I don't think you talk about the second threshold below. Maybe worth being explicit about this and explaining why. (I'm guessing your thinking is that if some part of CSER clears the first bar, then there's no point in checking whether it clears the second)

Yes, thanks 

* Page #8 (2.2. A threshold based on not leaving a Pareto improvement on the table): MATS do work other than MechInterp - do you have in mind just the MechInterp part of MATS? (Or maybe you think that all MATS work is robustly good, similar to MechInterp?)

Reworded, I just care about mechanistic interpretability here because I view it as having a low downside.

* Page #8 (2.2. A threshold based on not leaving a Pareto improvement on the table): My preference would be for a bit more detail on how you got these numbers. Is it making an estimate of how much MATS has reduced x-risk and dividing that by the cost of running MATS?

As you mention this section is just a refresher; you can see the background for the thresholds here: <https://github.com/NunoSempere/SoGive-CSER-evaluation/tree/master/3-drafts/0-thresholds/1-draft>. I've also updated that document with comments from the last conversation, so it should be a bit clearer as well.

* Page #11 (2.3. Comparison with other distributions and dominance criteria):

Section 2 seems good, but I got confused because it seems like the section is basically describing what decision procedure we should use but then you briefly mention in 2.3.1 and 2.3.2 that CSER is stochastically dominant over the AI safety community and above the first-principles threshold. (E.g. from the titles of those sections, I'd be surprised to find the answers here)

These seem like key results so I think you should flag them more explicitly / prominently. (Either do that in section 2, or avoid giving this answer to a subsequent section.)

Answer: let's talk about this in the call.

* Page #11 (3. A model of CSER's impact): I'm not sure here whether by "CSER" you mean the entire centre or just the AI part

I do mean just the AI part, clarified.

* Page #11 (3.1. Explanation of the model): (Minor) Maybe spell out a bit more what you mean by "influence" here - is it 'magnitude of reduction of x-risk from outside the UK that is attributable to CSER'?

> is it 'magnitude of reduction of x-risk from outside the UK that is attributable to CSER
yes


* Page #12 (3.1. Explanation of the model): NB I haven't had time to check these models

* Page #13 (3.2. Result of the model): (Minor) Would be helpful to label what these values are. (Cost per basis point in millions of USD, right?)

Good point, no, this is the inverse, basis points per million USD.

* Page #13 (3.2. Results of the model compared to the strict first principles threshold): NB: This is just repeating the table from immediately above

Thanks, this was a markdown error.

* Page #14 (3.2. Results of the model compared to the strict first principles threshold): Looks like the graph is missing or in the wrong place here

Thanks, added.

* Page #14 (3.2. Results of the model compared to the strict first principles threshold): I feel confused by this - isn't [AI part of CSER] better than [CSER as a whole] bc the EV of the AI part is higher, not just because of how we think about risk neutrality? (I interpret "the argument becomes a bit clearer" to mean something like the latter)

I wasn't making myself clear. 

- My default model includes some default chance of some funging between different parts of CSER
- If we don't include that funging, the value of donating to CSER AI has a more visible tail

I'll just say that explicitly in the next draft.

* Page #16 (3.3. Some commentary about what this means and where CSER is at.): Based just on reading this section (and not e.g. your other notes), the analysis here seems reasonable. For clarity, I recommend splitting this section into two: 'How I model CSER's impact' and then 'What this means'

Ok. I also then moved "How I model CSER's impact" to the beginning of section 3.

* Page #16 (3.3. Some commentary about what this means and where CSER is at.): (minor) For all of these graphs, would be helpful to say which color means what

Good point, might fix.

* Page #16 (3.3. Some commentary about what this means and where CSER is at.): interesting that CSER is better than robust technical AI safety donations, but not clearly better than the first principles X-risk-reduction threshold. Does this mean that robust technical AI safety does not clear the first principles threshold? This seems like an important and surprising result!

The first principles threshold is fairly strong here, yes.

* Page #17 (3.3. Some commentary about what this means and where CSER is at.):

Matthijs. I'm not sure I'm understanding correctly, here. Is your claim that Matthijs left CSER bc CSER didn't have enough funding, and thus that the 'additional marginal funding may be less effective' point doesn't apply?

Yes, but probabilistically. Can talk about this during the call. 

* Page #17 (3.3. Some commentary about what this means and where CSER is at.): typo - worlds

* Page #18 (4. Conclusion): This take seems reasonable, given the analysis above!

* Page #18 (A. Millions of dollars per basis point vs basis points per million dollars.): Based on skimming, appendices all seem reasonable

